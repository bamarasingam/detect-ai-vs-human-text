{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ac0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa3f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63432014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3751, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit = pd.read_csv('reddit_ask_question.csv')\n",
    "reddit.head(20)\n",
    "reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c2e8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to ask a question and get a response\n",
    "#https://www.linkedin.com/pulse/openai-completions-api-complete-guide-bo%C5%A1ko-bezik/\n",
    "#Referred to breakfast hour with Eric and docs\n",
    "def chat_with_gpt3(question):\n",
    "    response = openai.Completion.create(\n",
    "        engine = \"text-davinci-003\", #OpenAI Model\n",
    "        prompt = question, #Question to ask\n",
    "        temperature = 0.6, #Randomness within answer (sampling temp)\n",
    "        max_tokens = 500, #Adjust max_tokens based on the desired response length\n",
    "        #n = 20 #Number of questions in a batch\n",
    "\n",
    "    )\n",
    "    #The below line was taken from docs to extract the response\n",
    "    return response['choices'][0]['text'] ##https://platform.openai.com/docs/guides/gpt/chat-completions-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab93c8",
   "metadata": {},
   "source": [
    "I originally made my code without batching in mind. The way I made it was to have a function which can be called, which is given a list of questions, and individually go through each question then receive an answer. Once I found out about batching I tried to implement it into my code. I included n=20 for a batch size without editing/changing any of the other code so I hit my rate limit and was charged much more than I anticapted. Also too it took much more time on my other data (openai_q_and_a) when I used batching vs. individually asking questions.\n",
    "\n",
    "https://platform.openai.com/docs/guides/rate-limits/error-mitigation\n",
    "https://platform.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model\n",
    "\n",
    "The docs talked about optimizing but due to constraint on time, I just went with the original way I had it set up rather than trying to batch my questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "710e667b",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "Request failed due to server shutdown {\n  \"error\": {\n    \"message\": \"Request failed due to server shutdown\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'Request failed due to server shutdown', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sun, 24 Sep 2023 17:00:35 GMT', 'Content-Type': 'application/json', 'Content-Length': '141', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-davinci-003', 'openai-organization': 'big-shooter-golf', 'openai-processing-ms': '1543', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-limit-tokens_usage_based': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249500', 'x-ratelimit-remaining-tokens_usage_based': '249500', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '120ms', 'x-ratelimit-reset-tokens_usage_based': '120ms', 'x-request-id': 'ecacf91836a2dcdf2a8658bbfc83f403', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80bc95996b8fa232-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Loop through the questions and get answers\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[0;32m----> 9\u001b[0m     answer \u001b[38;5;241m=\u001b[39m chat_with_gpt3(question)\n\u001b[1;32m     10\u001b[0m     answers\u001b[38;5;241m.\u001b[39mappend(answer)\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mchat_with_gpt3\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_with_gpt3\u001b[39m(question):\n\u001b[0;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      6\u001b[0m         engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-003\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#OpenAI Model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m question, \u001b[38;5;66;03m#Question to ask\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;66;03m#Randomness within answer (sampling temp)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, \u001b[38;5;66;03m#Adjust max_tokens based on the desired response length\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m#n = 20 #Number of questions in a batch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m#Kept hitting rate limit when attempting to batch. Also took longer time to process\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#The below line was taken from docs to extract the response\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    702\u001b[0m             result\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAPIError\u001b[0m: Request failed due to server shutdown {\n  \"error\": {\n    \"message\": \"Request failed due to server shutdown\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'Request failed due to server shutdown', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sun, 24 Sep 2023 17:00:35 GMT', 'Content-Type': 'application/json', 'Content-Length': '141', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-davinci-003', 'openai-organization': 'big-shooter-golf', 'openai-processing-ms': '1543', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-limit-tokens_usage_based': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249500', 'x-ratelimit-remaining-tokens_usage_based': '249500', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '120ms', 'x-ratelimit-reset-tokens_usage_based': '120ms', 'x-request-id': 'ecacf91836a2dcdf2a8658bbfc83f403', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80bc95996b8fa232-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}"
     ]
    }
   ],
   "source": [
    "# Get the questions from the 'title' column of the DataFrame\n",
    "questions = reddit['title'].tolist()\n",
    "\n",
    "# List to store the answers\n",
    "answers = []\n",
    "\n",
    "# Loop through the questions and get answers\n",
    "for question in questions:\n",
    "    answer = chat_with_gpt3(question)\n",
    "    answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c7f7d",
   "metadata": {},
   "source": [
    "Ended up getting a server shutdown error. Will rerun, but going too break my questions into seperate lists and collect the data like that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e008ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the questions from the 'title' column of the DataFrame\n",
    "questions1 = reddit['title'].tolist()[:500]\n",
    "questions2 = reddit['title'].tolist()[500:1000]\n",
    "questions3 = reddit['title'].tolist()[1000:1500]\n",
    "questions4 = reddit['title'].tolist()[1500:2000]\n",
    "questions5 = reddit['title'].tolist()[2000:2500]\n",
    "questions6 = reddit['title'].tolist()[2500:3000]\n",
    "questions7 = reddit['title'].tolist()[3000:3500]\n",
    "questions8 = reddit['title'].tolist()[3500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c78e2",
   "metadata": {},
   "source": [
    "I kept getting some sort of error from the server side so breaking the questions down into smaller parts so that if it executes the first 600, but then has an error, at least I would have those captured and saved.\n",
    "\n",
    "In hindsight, could've looped these sections and if the code did break, perhaps it finished the first 3 sections and broke on the 4th, so I would still have those first 3 saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "728255fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the answers\n",
    "answers1 = []\n",
    "\n",
    "# Loop through the questions and get answers\n",
    "for question in questions1:\n",
    "    answer = chat_with_gpt3(question)\n",
    "    answers1.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5963887e",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "Request failed due to server shutdown {\n  \"error\": {\n    \"message\": \"Request failed due to server shutdown\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'Request failed due to server shutdown', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sun, 24 Sep 2023 17:39:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '141', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-davinci-003', 'openai-organization': 'big-shooter-golf', 'openai-processing-ms': '3093', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-limit-tokens_usage_based': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249500', 'x-ratelimit-remaining-tokens_usage_based': '249500', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '120ms', 'x-ratelimit-reset-tokens_usage_based': '120ms', 'x-request-id': '963b88022075812fbc2878ad92436da6', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80bccf2a8d0e36b4-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m answers2 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions2:\n\u001b[0;32m----> 4\u001b[0m     answer \u001b[38;5;241m=\u001b[39m chat_with_gpt3(question)\n\u001b[1;32m      5\u001b[0m     answers2\u001b[38;5;241m.\u001b[39mappend(answer)\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mchat_with_gpt3\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_with_gpt3\u001b[39m(question):\n\u001b[0;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      6\u001b[0m         engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-003\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#OpenAI Model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m question, \u001b[38;5;66;03m#Question to ask\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;66;03m#Randomness within answer (sampling temp)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, \u001b[38;5;66;03m#Adjust max_tokens based on the desired response length\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m#n = 20 #Number of questions in a batch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m#Kept hitting rate limit when attempting to batch. Also took longer time to process\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#The below line was taken from docs to extract the response\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    702\u001b[0m             result\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    766\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    767\u001b[0m     )\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAPIError\u001b[0m: Request failed due to server shutdown {\n  \"error\": {\n    \"message\": \"Request failed due to server shutdown\",\n    \"type\": \"server_error\",\n    \"param\": null,\n    \"code\": null\n  }\n}\n 500 {'error': {'message': 'Request failed due to server shutdown', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Sun, 24 Sep 2023 17:39:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '141', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'text-davinci-003', 'openai-organization': 'big-shooter-golf', 'openai-processing-ms': '3093', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '250000', 'x-ratelimit-limit-tokens_usage_based': '250000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '249500', 'x-ratelimit-remaining-tokens_usage_based': '249500', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '120ms', 'x-ratelimit-reset-tokens_usage_based': '120ms', 'x-request-id': '963b88022075812fbc2878ad92436da6', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '80bccf2a8d0e36b4-YYZ', 'alt-svc': 'h3=\":443\"; ma=86400'}"
     ]
    }
   ],
   "source": [
    "answers2 = []\n",
    "\n",
    "for question in questions2:\n",
    "    answer = chat_with_gpt3(question)\n",
    "    answers2.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20859993",
   "metadata": {},
   "source": [
    "Another server error, will redo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "191ba017",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers2 = []\n",
    "\n",
    "for question in questions2:\n",
    "    answer = chat_with_gpt3(question)\n",
    "    answers2.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0822835e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d2fdccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers3 = []\n",
    "\n",
    "for question in questions3:\n",
    "    answer = chat_with_gpt3(question)\n",
    "    answers3.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1810a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba6785b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers4 = []\n",
    "\n",
    "for question in questions4:\n",
    "    answer = chat_with_gpt3(question)\n",
    "    answers4.append(answer)\n",
    "    \n",
    "len(answers4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "830502db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers5 = []\n",
    "\n",
    "for question in questions5:\n",
    "    answer = chat_with_gpt3(question)\n",
    "    answers5.append(answer)\n",
    "    \n",
    "len(answers5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2feb9f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers6 = []\n",
    "\n",
    "for question in questions6:\n",
    "    answer = chat_with_gpt3(question)\n",
    "    answers6.append(answer)\n",
    "    \n",
    "len(answers6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "690d9cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers7 = []\n",
    "\n",
    "for question in questions7:\n",
    "    answer = chat_with_gpt3(question)\n",
    "    answers7.append(answer)\n",
    "    \n",
    "len(answers7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fc17afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers8 = []\n",
    "\n",
    "for question in questions8:\n",
    "    answer = chat_with_gpt3(question)\n",
    "    answers8.append(answer)\n",
    "    \n",
    "len(answers8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3368cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = answers1 + answers2 + answers3 + answers4 + answers5 + answers6 + answers7 + answers8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d20e033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit['answers'] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c106249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>top_comment</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I purposely get clumps in my spaghetti</td>\n",
       "      <td>I’ve actually done this before. \\n\\nTie a whol...</td>\n",
       "      <td>sauce?\\n\\n1. Use a potato masher to mash the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rabbit hole engaged... 11,082 cookbooks in dig...</td>\n",
       "      <td>Brewster Kahle is the man responsible for this...</td>\n",
       "      <td>\\n\\nThere are actually a few digital collectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello everybody, I’m Maangchi! I have a Korean...</td>\n",
       "      <td>Hello Maangchi! I've followed your channel sin...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UPDATE: I prepared Japanese A5 Wagyu Steak for...</td>\n",
       "      <td>This is my favorite type of update post! All t...</td>\n",
       "      <td>!\\n\\nThank you so much for your help! I really...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Instacart gave me 50 pounds of limes instead o...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>\\n\\nThere are many things you can do with 50 p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0     How can I purposely get clumps in my spaghetti   \n",
       "1  Rabbit hole engaged... 11,082 cookbooks in dig...   \n",
       "2  Hello everybody, I’m Maangchi! I have a Korean...   \n",
       "3  UPDATE: I prepared Japanese A5 Wagyu Steak for...   \n",
       "4  Instacart gave me 50 pounds of limes instead o...   \n",
       "\n",
       "                                         top_comment  \\\n",
       "0  I’ve actually done this before. \\n\\nTie a whol...   \n",
       "1  Brewster Kahle is the man responsible for this...   \n",
       "2  Hello Maangchi! I've followed your channel sin...   \n",
       "3  This is my favorite type of update post! All t...   \n",
       "4                                          [deleted]   \n",
       "\n",
       "                                             answers  \n",
       "0   sauce?\\n\\n1. Use a potato masher to mash the ...  \n",
       "1  \\n\\nThere are actually a few digital collectio...  \n",
       "2                                                     \n",
       "3  !\\n\\nThank you so much for your help! I really...  \n",
       "4  \\n\\nThere are many things you can do with 50 p...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09abaf2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3751, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7bf367ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.to_csv('reddit_chatgpt_df.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
